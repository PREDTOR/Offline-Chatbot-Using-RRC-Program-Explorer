{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbdc531b-b154-48df-8499-d28d4a2da4cc",
   "metadata": {},
   "source": [
    "<h1 align=center >Chatbot Project </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8d31e-fb26-498e-9191-35944ae59068",
   "metadata": {},
   "source": [
    "### Final Project - Large Language Model Chatbot\n",
    "\n",
    "**Group 1** - Cassandra Phung, Kumar Kartikeyn Arora, Husandeep Kaur<br>\n",
    "**Course** - COMP-3704 (254805) Neural Networds and Deep Learning<br>\n",
    "**Date** - December 5, 2024\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "**Project Information**<br>\n",
    "This chatbot was developed to operate entirely locally and offline, ensuring user privacy and data security. It enables users to load PDF files as input and interact with a language model (LLM) directly within this Jupyter Notebook.<br>\n",
    "\n",
    "**Use Case**<br>\n",
    "For this demonstration, the chatbot processes information from three of Red River College's Information Technology Programs. The data has been sourced directly from the official RRC website.<br>\n",
    "\n",
    "**Other Potential Applications**<br>\n",
    "This chatbot is particularly useful for scenarios involving sensitive data that users prefer not to upload to online platforms, as many companies use user-uploaded data to improve their generative AI.a\n",
    "\n",
    "**Examples of sensitive files include:**\n",
    "- Patient Information: Medical records or other health-related documents.\n",
    "- Financial Information: Banking or investment details.\n",
    "- Personal Information: Documents users wish to keep private.\n",
    "- By keeping the process offline, this tool provides a secure alternative for handling confidential files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618ce380-7af2-49ce-8c38-2c57c6c38ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91932dee-29ad-4aff-9597-704be7211180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting environment variable for compatibility\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f46704-3bbd-4796-92ef-a88002909172",
   "metadata": {},
   "source": [
    "**Instructions**<br>\n",
    "Ensure ollama is running, as models need to communicate with the server\n",
    "- Open command prompt\n",
    "- Check if ollama is running: ollama --version\n",
    "- If it cannot connect to a running Ollama instance: ollama start\n",
    "- Open new command prompt\n",
    "- ollama list\n",
    "- Ensure you have both models: llama3.2 and nomic-embed-text:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb967e5-8e8e-4e88-a4eb-3f020565418f",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Loading PDF\n",
    "### LangChain document loader: UnstructuredPDFLoader\n",
    "- Extracts text from PDF documents making it ready for further processing (like chunking & embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316f692c-c88a-426f-88f7-ff03da49b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for the PDFs\n",
    "# Loading multuple PDFs\n",
    "# Each pdf containing information of an RRC Information Technology Program\n",
    "pdf_files = [\n",
    "    \"Information_Security.pdf\",\n",
    "    \"Data_Science_Machine_Learning.pdf\",\n",
    "    \"Application_Development_Delivery.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb88eb1-a6a5-429b-b996-b5fae80f00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map file names to user-friendly program names\n",
    "def program_name_mapper(file_name):\n",
    "    program_map = {\n",
    "        \"Information_Security\": \"Information Security Program\",\n",
    "        \"Data_Science_Machine_Learning\": \"Data Science & Machine Learning Program\",\n",
    "        \"Application_Development_Delivery\": \"Application Development & Delivery Program\"\n",
    "    }\n",
    "    return program_map.get(file_name.split(\".\")[0], \"Unknown Program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4028d5f4-55dd-4421-930c-73510899f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents and add metadata\n",
    "# Function to load and process PDFs\n",
    "\n",
    "def process_pdfs(file_paths):\n",
    "    document_collection = []\n",
    "    \n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            loader = UnstructuredPDFLoader(file_path=path)\n",
    "            documents = loader.load()\n",
    "            print(f\"Loaded PDF: {path}\")\n",
    "\n",
    "            # Add metadata to each document\n",
    "            # Metadata identifying which program a document belongs to\n",
    "            # Easier to filter results by program when retrieving or answering queries\n",
    "            for doc in documents:\n",
    "                \n",
    "                # Create metadata dictionary for each document\n",
    "                doc.metadata = {\n",
    "                    \"program\": path.split(\".\")[0],\n",
    "                    \"program_name\": program_name_mapper(path),\n",
    "                    \"source\": \"Program PDF\"\n",
    "                }\n",
    "                document_collection.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {e}\")\n",
    "    return document_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed3c240-58c4-4abb-85e7-eb59eff89104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PDF: Information_Security.pdf\n",
      "Loaded PDF: Data_Science_Machine_Learning.pdf\n",
      "Loaded PDF: Application_Development_Delivery.pdf\n"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "documents = process_pdfs(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad9ef2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'program': 'Information_Security', 'program_name': 'Information Security Program', 'source': 'Program PDF'}\n",
      "{'program': 'Data_Science_Machine_Learning', 'program_name': 'Data Science & Machine Learning Program', 'source': 'Program PDF'}\n",
      "{'program': 'Application_Development_Delivery', 'program_name': 'Application Development & Delivery Program', 'source': 'Program PDF'}\n"
     ]
    }
   ],
   "source": [
    "# Preview the first 3 documents\n",
    "for doc in documents[:3]:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd3b0255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL-TIME | WINNIPEG LOCATIONS\n",
      "\n",
      "1: Information Security\n",
      "\n",
      "Overview\n",
      "\n",
      "\n",
      "\n",
      "Two-year advanced diploma\n",
      "\n",
      "Fall entry date\n",
      "\n",
      "Classes take place between 8 am and 6 pm\n",
      "\n",
      "Classes are held on campus on Mondays, Tuesdays, and Wednesday mornings and online\n",
      "\n",
      "on Wednesday afternoons, Thursdays, and Fridays\n",
      "\n",
      "Exchange District Campus (formerly Princess Street Campus), Winnipeg\n",
      "\n",
      "Co-op work experience\n",
      "\n",
      "\n",
      "\n",
      "International app\n"
     ]
    }
   ],
   "source": [
    "# Preview the text content of the first document (first 400 characters)\n",
    "print(documents[0].page_content[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b09b18-0221-4b4f-a9b1-0a2812de8c57",
   "metadata": {},
   "source": [
    "---\n",
    "**Using multiple PDFs instead of 1.**<br> \n",
    "Splitting PDFs independantly by program helps avoid confusion with the chatbot.\n",
    "\n",
    "- Allows for clearer segmentation of the content\n",
    "- Less risk of confusing similar content across programs\n",
    "- Easier to query specific content without possible overlap (similar terms used across programs, fees, dates, courses, year, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887c025-e6b7-48d6-8d3f-3bde357f75b5",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Splitting and Chunking Text\n",
    "\n",
    "**RecursiveCharacterTextSplitter**\n",
    "- Splits long documents into smaller coherent pieces making sure that chunks don't break at awkward places (ex. in the middle of a sentence).\n",
    "- Document requires text splitting because one full document may be too large or difficult to process for LLMs.\n",
    "- chunk_size means each chunk will contain up to the declared number of characters.\n",
    "- chunk_overlap means each chunk will overlap the previous chunk by the declared number of characters.\n",
    "- This overlap helps ensure that important information, that may be split between chunks, remains in both chunks.\n",
    "- Larger chunks give better Context Retention as larger chunks mean LLM gets more context.\n",
    "- Improving retrieval effecnciency by reducing embedding computation which will have fewer but more meaningful chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e56b081-e370-4d32-8a64-100b281dcd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total document chunks created: 44\n"
     ]
    }
   ],
   "source": [
    "# Split the documents into manageable chunks\n",
    "\n",
    "# Changing Chunk size to a higher number, Now more content in a single chunk is taken.\n",
    "# Overlap 200, to avoid losing key details\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "doc_chunks = splitter.split_documents(documents)\n",
    "print(f\"Total document chunks created: {len(doc_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c71d18-1624-4674-b5f4-b85dc6905b1c",
   "metadata": {},
   "source": [
    "#### NOTES\n",
    "**Attempt 4**<br>\n",
    "Reduce redundancy in the embeddings\n",
    "- Ensure each chunk embedded has enough unique content, if the overlap is too big, the model can get confused since each chunk will be very similar. If the overlap is too small, there won't be enough context shared between chunks and the content may not make sense since it can stop at the middle of a sentence.\n",
    "- Chunk size: larger chunks include more content, smaller chunks allow fore more precise embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ff97c-d39d-4b1d-94eb-178cc5dca66a",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Embeddings and Storing in Vector Database\n",
    "\n",
    "- Use Ollama from this Jupyter Notebook.\n",
    "- Sends text to the locally running Ollama service for embedding, where embedding model: nomic-embed-text converts the chunks into vectors.\n",
    "- Vector database (vector_db) has been created using Chroma and will store the embeddings.\n",
    "- Allowing for an efficient search of relevant information from user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f646d3-c70d-4a22-af56-d2b4ffd7955d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create vector database\n",
    "# Embed and store chunks into vector database where it will be later retrieved\n",
    "vector_storage = Chroma.from_documents(\n",
    "    documents=doc_chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    collection_name=\"program-advisor-db\"\n",
    ")\n",
    "\n",
    "print(\"Vector database created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b13b52-bc45-469a-b232-257ade6ad83f",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Set up LLM and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ade96043-a62b-4f02-96aa-ac32b5741541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Llama 3.2.1b model\n",
    "chat_model = ChatOllama(\n",
    "    model=\"llama2:7b\", \n",
    "    base_url=\"http://localhost:12345\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680079b0-e3e4-4868-9f40-04793d747145",
   "metadata": {},
   "source": [
    "The port default for ollama on PC is 11434, if error encountered:change code to \"http://localhost:11434\"\n",
    "\n",
    "**LLM Selection and Impact on Performance**\n",
    "- Original model llama2:7b\n",
    "- Alternative model llama3.2:1b\n",
    "- The original model has 7 billion parameters while the new model has 1 billion parameters, allowing the model to output faster responses with the tradeoff of performance and accuracy\n",
    "- Final chatbot LLM selection: llama2:7b since the responses were more contextually accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be165a6-a4e1-40d1-a752-bc32b0841aa8",
   "metadata": {},
   "source": [
    "**QUERY_PROMPT template** is required and an established template used for setting up the LLM to generate multiple versions of the user's query.\n",
    "- This part does not involve embedding the user query yet, it's just to generate alternative versions/phrasings of the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9957bb04-ba62-482b-a664-75acac763c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Question: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c753f-c484-42f2-bc94-bc9342fc8a4d",
   "metadata": {},
   "source": [
    "Instead of just using the original query as is, the **MultiQueryRetriever** generates multiple different versions of the query. All versions of the query are sent to the vector database to retrieve matches to any of the versions of the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921ee86-b0f2-411c-8be8-739c9d971fc3",
   "metadata": {},
   "source": [
    "By using MultiQueryRetriever:\n",
    "- vector_storage.as_retriever() - retrieves from the vector database\n",
    "- chat_model - large language model that is used to generate the different versions of the user queries\n",
    "- prompt=template - the prompt template created used to create multiple versions of the user query\n",
    "\n",
    "These versions of the user queries are not directly embedded yet, but after they are generated by the LLM, they will be embedded using the same embedding=OllamaEmbeddings(model=\"nomic-embed-text\") model that was used earlier for the PDFs. This step happens automatically when using MultiQueryRetriever so there is no need to code for the user query embedding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1b49f0b-9930-44bf-a3cc-e677e00d258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MultiQueryRetriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_storage.as_retriever(), \n",
    "    chat_model,\n",
    "    prompt=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f766d089-4dc5-4201-8ef1-7b8c164b5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved prompt template with placeholders for program\n",
    "response_prompt = PromptTemplate(\n",
    "    input_variables=[\"program_name\", \"context\", \"user_query\"],\n",
    "    template=\"\"\"\n",
    "    You are an academic advisor. Answer the user's query concisely based only on the provided context.\n",
    "\n",
    "    Program: {program_name}\n",
    "    Context: {context}\n",
    "    Query: {user_query}\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4631ebd-97fb-450a-82e7-d0aac954f8ff",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Processing user Queries & Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ef6c09b-1d3d-4942-b8c0-b15796338470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context_by_program(query, program_filter):\n",
    "    # Prepare the inputs for the retriever\n",
    "    prompt_inputs = {\n",
    "        \"program\": program_filter,\n",
    "        \"query\": query\n",
    "    }\n",
    "\n",
    "    # Generating relevant documents using MultiQueryRetriever\n",
    "    results = retriever.get_relevant_documents(prompt_inputs)\n",
    "\n",
    "    # Extracting and combine relevant context\n",
    "    context = \"\\n\".join([\n",
    "        doc.page_content for doc in results if program_filter in doc.metadata.get(\"program\", \"\")\n",
    "    ])\n",
    "    return context if context else \"No specific information available.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44e087c8-94f1-4ce0-94a0-70cadece8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "def generate_response(query, program_filter):\n",
    "    # Fetch context and program name\n",
    "    context = fetch_context_by_program(query, program_filter)\n",
    "    if not context.strip():\n",
    "        return \"No specific information is available in the provided documents for this query.\"\n",
    "\n",
    "    program_name = program_name_mapper(program_filter)\n",
    "\n",
    "    # Prepare structured messages\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an academic advisor.\"),\n",
    "        HumanMessage(content=f\"Program: {program_name}\\nQuery: {query}\\nContext: {context}\")\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Generate response\n",
    "        response = chat_model(messages, max_tokens=150)\n",
    "        return response.content.strip()\n",
    "    except Exception as e:\n",
    "        return \"An error occurred while generating the response. Please try again.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe48e1-de6b-429f-bb0c-afdbd2659930",
   "metadata": {},
   "source": [
    "### 6. Chatbot interactive function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b866b-c34d-45da-98a7-a981451ca0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Academic Advisor Chatbot!\n",
      "Type 'exit' to quit the chatbot.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question:  tell me about the data science and machine learning program\n",
      "Specify the program (e.g., Data_Science_Machine_Learning):  data science and machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot Response:\n",
      "Hello there! I'm happy to help you with any questions you may have about our data science and machine learning program. Unfortunately, I don't have access to specific information about the program as it is not provided in the context of our conversation. Could you please provide more details or context about the program you are interested in? This will help me better understand your question and provide a more accurate answer.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question:  give me an overview on data science and machine learning program\n",
      "Specify the program (e.g., Data_Science_Machine_Learning):  data science and machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot Response:\n",
      "As an academic advisor, I must provide you with a comprehensive overview of the Data Science and Machine Learning (DSML) program, highlighting its key components, application areas, and potential career paths. However, since no specific information is available, I will outline a general framework for the program. Please note that the details may vary depending on the institution offering the program.\n",
      "\n",
      "Data Science and Machine Learning Program:\n",
      "\n",
      "1. Coursework: The DSML program typically includes courses in data mining, machine learning, statistical modeling, computer programming, and data visualization. Students will learn to collect, analyze, and interpret large datasets, as well as develop algorithms and models to solve complex problems.\n",
      "2. Core Courses:\n",
      "a. Data Mining and Machine Learning Fundamentals: Introduction to data mining and machine learning techniques, including supervised and unsupervised learning, deep learning, and neural networks.\n",
      "b. Statistical Modeling and Probability Theory: Exposure to statistical modeling and probability theory, which provides the mathematical foundation for machine learning algorithms.\n",
      "c. Data Visualization and Communication: Techniques for effectively communicating insights and findings through data visualization and storytelling.\n",
      "d. Computer Programming and Scripting: Proficiency in programming languages such as Python, R, or MATLAB, and scripting tools like SQL and MongoDB.\n",
      "3. Elective Courses: Students can choose from a range of electives to specialize in specific areas, such as:\n",
      "a. Natural Language Processing (NLP): Focus on techniques for processing and analyzing natural language data, including text analysis, sentiment analysis, and topic modeling.\n",
      "b. Recommendation Systems: Study algorithms and techniques used in recommendation systems, including collaborative filtering, content-based filtering, and matrix factorization.\n",
      "c. Time Series Analysis: Learn to analyze and forecast time series data, including ARIMA models, seasonal decomposition, and machine learning approaches.\n",
      "d. Data Engineering: Explore the design and implementation of large-scale data systems, including data warehousing, ETL (Extract, Transform, Load) processes, and data pipelines.\n",
      "4. Projects and Capstone: Students will work on real-world projects or a capstone project to apply their knowledge and skills in a practical setting. This provides an opportunity to demonstrate their ability to solve complex problems using data science and machine learning techniques.\n",
      "5. Practical Applications: Data Science and Machine Learning are applied across various industries, including:\n",
      "a. Healthcare: Analyzing medical records, identifying disease risk factors, and developing personalized treatment plans.\n",
      "b. Finance: Predicting stock prices, detecting fraud, and optimizing investment portfolios.\n",
      "c. Marketing: Understanding customer behavior, predicting sales, and recommending targeted marketing campaigns.\n",
      "d. Transportation: Optimizing route planning, predicting traffic patterns, and improving vehicle safety.\n",
      "6. Career Paths: Graduates of the DSML program can pursue various career paths, including:\n",
      "a. Data Scientist/Machine Learning Engineer: Developing and implementing machine learning models to solve complex problems in various industries.\n",
      "b. Data Analyst/Data Engineer: Working with large datasets to extract insights and build data systems.\n",
      "c. AI/Deep Learning Specialist: Focusing on the development and deployment of deep learning models for applications such as computer vision, natural language processing, and speech recognition.\n",
      "d. Business Analyst/Consultant: Providing data-driven solutions to businesses by analyzing data, identifying opportunities, and developing strategies.\n",
      "e. Academic Researcher: Pursuing research in machine learning, data science, or a related field, contributing to the advancement of knowledge and understanding in these areas.\n",
      "\n",
      "In summary, the Data Science and Machine Learning program provides students with a comprehensive education in data-driven problem-solving, equipping them with the skills and knowledge necessary to excel in various industries and careers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interactive chatbot loop for dynamic queries\n",
    "def chatbot_interactive():\n",
    "    print(\"Welcome to the Academic Advisor Chatbot!\")\n",
    "    print(\"Type 'exit' to quit the chatbot.\")\n",
    "    while True:\n",
    "        try:\n",
    "            user_query = input(\"Enter your question: \")\n",
    "            if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            program_filter = input(\"Specify the program (e.g., Data_Science_Machine_Learning): \").strip()\n",
    "            if not program_filter:\n",
    "                print(\"Please specify a valid program.\")\n",
    "                continue\n",
    "            response = generate_response(user_query, program_filter)\n",
    "            print(f\"\\nChatbot Response:\\n{response}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c04c4-e5fc-4386-84f5-152b64ab246d",
   "metadata": {},
   "source": [
    "<h1 align=center >End </h1>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
